Microservices and Distributed Systems
    https://app.amigoscode.com/p/microservices

    https://spring.io/cloud

    you can create a banner for your spring app here:
        https://devops.datenkollektiv.de/banner.txt/index.html

To start docker:
     docker compose up -d      (-d for detached)


1. Generally, ideally, each microservice has its own database. Since we are in developing mode,
   and our laptop resources are limited, I will use the same container, with multiple DBs
   instead of having one DB per container.
   https://stackoverflow.com/questions/57421950/why-does-each-microservice-get-its-own-database

2. Since we use lombok, in each bean, like service, controller etc, if we annotate it with
    @AllArgsConstructor, then a constructor will be generated behind the scenes with all fields.
    And by having this, spring will know to search for those fields, so tha they will be injected.
    If we wouldn't use the annotation, we could manually create the constructors, or use
    @Autowired.

3. @Slf4j is a lombok annotation that adds a logger field to your class

4. why saveAndFlush is needed in some scenarios like the one used in CustomerService:
    https://www.baeldung.com/spring-data-jpa-save-saveandflush

5. http://localhost:8761/
    to see the webconsole of eureka server ... this is of course after running the EurekaServerApplication

6. Eureka version 4.0.0 onwards, which is being used in Spring Cloud 2022.0.0, you do not need to explicitly register
   using the annotation @EnableEurekaClient It automatically gets registered as client if
   spring-cloud-starter-netflix-eureka-client is on the class path. As per the - documentation
        By having spring-cloud-starter-netflix-eureka-client on the classpath, your application
        automatically registers with the Eureka Server. Configuration is required to locate the Eureka server

7. "http://FRAUD/api/v1/fraud-check/{customerId}"
    Means that eureka must be running...if you open console (see point 5) you will see there that application
    name FRAUD has an address corresponding to it, which will be used instead of FRAUD

    // one way of calling the fraud microservice
            FraudCheckResponse fraudCheckResponse = restTemplate.getForObject(
                    "http://FRAUD:8081/api/v1/fraud-check/{customerId}",
                    FraudCheckResponse.class,
                    customer.getId()
                    );

8. @LoadBalanced tells spring that the request being made can go to any of the registered eureka apps
    (for example the one from CustomerConfig, when used with
    "http://FRAUD:8081/api/v1/fraud-check/{customerId}") will go to any FRAUD app

9. Concerning the @FeignClient annotation
    https://docs.spring.io/spring-cloud-openfeign/reference/spring-cloud-openfeign.html

    "In the @FeignClient annotation the String value ("stores" above) is an arbitrary client name,
    which is used to create a Spring Cloud LoadBalancer client. You can also specify a URL using the
    url attribute (absolute value or just a hostname). The name of the bean in the application context is the fully
    qualified name of the interface.
  !!!    The load-balancer client above will want to discover the physical addresses for the "stores" service. If your
    application is a Eureka client then it will resolve the service in the Eureka service registry.
        Dan: this last sentence it why we can just specify "FRAUD" in the annotation

10. Concerning tracing ...sleuth (which is used in the lessons) is discontinued for this version of spring boot.
    Replacing it is micrometer..
    Used the below link to understand how to do things mentioned in tutorial with micrometer
    https://openvalue.blog/posts/2022/12/16/tracing-in-spring-boot-2-and-3/

    !!! TODO I could not make it work :( ...maybe have a look over the SpringBootObservability, and use
    actuator and micrometer to get info in zipkin..

11. Concerning the LoadBalancer (API Gteway)...normally you would use the load balancer offered by Google,
    AWS etc...you will not create/configure one on your own machine...

12. Microservices should not be exposed to the open internet directly...they are usually protected by a firewall.
    Instead, all trafic goes through the load balancer. Which means that if up until now we made a request to
    the Customers microservice directly, http://localhost:8080/api/v1/customers, now we will go through
    the load balancer by using http://localhost:8083/api/v1/customers

13. We will use a message broker, Rabbit MQ, in order to handle the sending of notification. Right now, as the code is
    if for some reason, notification microservice takes 10s to process a request, then this will be a very bad
    user experience for the user. Since a notification like getting an email/SMS does not have to be instantaneous,
    the communication with the notification microservice will be done with a message broker....in this case
    Rabbit MQ.

14. When defining the Rabbit MQ, we specify 2 lines of ports, because some are for the microservices to connect to,
    while the second is for management port, so that you can use the management console.
              - "5672:5672"
              - "15672:15672"

15. For RabbitMQ UI console visit http://localhost:15672/. username and pass are guest/guest

16.  <artifactId>spring-boot-starter-amqp</artifactId>
        amqp comes from -> advanced message queue protocol

17. RabbitTemplate allows us to send messages
    SimpleRabbitListenerContainerFactory    allows us to receive messages from the queue

18. http://localhost:15672/ after all is setup, and you run the Eureka server and the Notification
    microservice (having a commandLineRunner in the application class that sends a message), if you
    go to Queues, and then click on Get Messages through the UI, you should be able to see the payload
    sent from the commandLineRunner

19. You can run the microservices from terminal by using the command (run this after you ran mvn package command)
    java -jar name_of_jar.jar

20. We will use Jib to create the docker images of our app
    https://github.com/GoogleContainerTools/jib

    In main pom we defined property that is used to create the image name. The first part
    "teshte" MUST be your docker hub username !!!
        <!-- this is used by Jib maven plugin to set the image version -->
        <image>teshte/${project.artifactId}:${project.version}</image>

     <!-- below plugin is used to create docker images -->
            <plugin>
              <groupId>com.google.cloud.tools</groupId>
              <artifactId>jib-maven-plugin</artifactId>
              <version>3.4.0</version>
              <configuration>
                <from>
                    <!-- this is the so called based image -->
                  <image>eclipse-temurin:17</image>
                </from>
                <to>
                  <tags>
                    <tag>latest</tag>
                  </tags>
                </to>
              </configuration>
            </plugin>


    Together with the definitions like the one from below that are in the microservice modules
        <profiles>
                <profile>
                    <id>build-docker-image</id>
                    <build>
                        <plugins>
                            <plugin>
                                <groupId>com.google.cloud.tools</groupId>
                                <artifactId>jib-maven-plugin</artifactId>
                                <executions>
                                    <execution>
                                        <phase>package</phase>
                                        <!-- goal from below will push our image to docker registry -->
                                        <goals>
                                            <goal>build</goal>
                                        </goals>
                                    </execution>
                                </executions>
                            </plugin>
                        </plugins>
                    </build>
                </profile>
            </profiles>

21. So to create and publish the image of a microservice on docker hub, select the profile
    "build-docker-image" and then run the "package" phase...(since you are logged in to docker
    hub because of the instalation/setup that i did some time ago)
        in case you get errors because of login issues, look for "docker login" and
        "docker logout" commands..

    You can run the "package" phase on a microservice module, like api-gateway, customer,
    fraud,notification, eureka-server, which will build and publish just the image of
    that microservice to docker hub, or just run it on the main, parent project, in which
    case all microservices docker images will be built and pushed to docker hub.


22. In docker-compose.yml we will add the services for each microservice... Notice that we need to put
        networks:
          - spring
    because we want the microservices to be able to communicate with each other

22. application-docker.yml are files that will be used when the "docker" spring profile is activated
    instead of localhost...these files have the name of the container in URLs...because since each
    microservice has its own container, it would not work with "localhost"

23. After you have your docker microservices set up, you can run from the parent folder the command
        docker compose up -d
    to start up all the services defined in docker-compose
        docker compose stop
    to stop all containers

24. Kubernetes aka K8S (8 comes from the number of letters between K and S) is an application
    orchestrator...and by application we mean container here

    master nodes - the brain
                 - this has several components:
                    -API server - this enabled communication between the below components, system administrator etc
                    -Scheduler
                    -Cluster store - where the configuration/data is stored
                    -Cloud Controller Manager - used to communicate with the cloud providers (AWS, Azzure, Google cloud)
                    -Controller manager
    worked nodes - nodes where the actual work of the app is done, the microservice nodes
                 - VM or physical machine running linux
                 - 3 main components:
                    -Kublet - main agent that runs on every node; recieves Pod definitions from master node's API server
                            - reports Pod state to master node through API Server
                            - interacts with Container Runtime to run containers associated with the Pod
                    -Container runtime - pulls images from container registries, like Docker Hub etc
                                       - starts and stops containers
                                       - CRI - container runtime interface - interface for 3rd party container runtime ..
                                            this is because kubernetes deprecated docker, and instead uses Containerd, which
                                            is the standard container runtime for kubernetes
                    -Kube proxy - responsible for each node getting its own unique IP address...and other things :)

    https://containerd.io/
    https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/

25. Running kubernetes is possible in 2 ways:
    1. run it yourself...which is very hard.. with solutions such as miniKube, kind, docker
            https://minikube.sigs.k8s.io/docs/start/
    2.managed kubernetes...using the AWS...Google etc infrastructure
        by managed it means that you as a dev, don't need to worry about running all the master node components, like
        API server, Scheduler etc...and you can just focus on the worker nodes

26. We will use miniKube to run kubernetes locally. Some useful commands:
    brew install minikube       - to install
    minikube version            - to see installed version
    minikube start --memory=4g  - to start minikube with 4 gb of RAM
    minikube stop               - obviously stops
    minikube status
    minikube ip                 - ip of master node

27. Kubectl - Kubernetes command line tool...lets us interact with the kubernetes cluster

28. A Pod is a collection of 1 or more containers (in the context of kubernetes)
        - a Pod is the smallest deployable unit in kubernetes...in docker the smallest deployable unit is a container
        - never create pods on its own..use controllers instead...because pods are disposable and
        temporal...meaning they should be controlled/managed from one place, not chaotically

29. Minikube kubectl commands part 1
    kubectl run hello-world --image=amigoscode/kubernetes:hello-world --port=80         Run a Pod in kubernetes
    kubectl get pods
    kubectl port-forward pod/hello-world 8080:80            - need this port forwarding so that we can access the pod from local machine..
                                                            - this is not something to be used in production...in production we use Services
                                                            to access the Pods
    kubectl delete pod hello-world                          - deletes a pod

30. Types of Kubernetes Services:
        ClusterIP (default)
        NodePort
        ExternalName
        LoadBalancer

31. !!!!!!!
    In production you should never deploy your DB in kubernetes in production...Sure, locally you can do.
    In production you should use a managed solution/database like Amazon Relational Database Service (RDS)..or from
    Google/Azzure

32. In /spring-microservices/kubernetes/minikube/bootstrap/postgres/volume.yml
    we specify how postgres will store data into our minikube node

    In spring-microservices/kubernetes/minikube/bootstrap/postgres/configmap.yml
    we specify the postgres config like user password DB name
    spring-microservices-db ...is just a dummy DB name. Since each microservice will have its own
    DB, these will be created manually...see point 33

    In spring-microservices/kubernetes/minikube/bootstrap/postgres/service.yml
    we specify how other Pods communicate with postgres

    In spring-microservices/kubernetes/minikube/bootstrap/postgres/statefulset.yml
    is needed to instantiate the instance
        the part
                      envFrom:
                        - configMapRef:
                            name: postgres-config
        means that configuration name is postgres-config which is present in configmap.yml and
        contains username password DB name

33. Minikube kubectl commands part 2
        kubectl apply -f bootstrap/postgres/            to create a Pod from your files..you run this from minikube folder
        kubectl get pods -w                             get status of local pods. you will see a column NAME, where the service
                                                        names are present...if you take the service name from there, you can
                                                        use the following command

        minikube service --url rabbitmq                 rabbitmq comes from previous command...this will output some URLs
                                                        that you can use to access the service running inside kubernetes. You
                                                        need to keep this command running, in order to access the service ..
                                                        this is mentioned in the result of the command

        kubectl get all                                 more information about local pods
        kubectl describe pod postgres-0                 gives more info about the Pod with that name
        kubectl logs postgres-0                         logs from Pod
        minikube ssh                                    gets you a terminal inside minikube
        kubectl exec -it postgres-0 -- psql -U postgres         connect to the postgres database server inside the Pod
                                                                if you execute \l you will see existing databases
                !!! from this terminal, we manually created the DBs:
                    create database customer;
                    create database fraud;
                    create database notification;

        kubectl describe pod customer-8f9b7ffcd-qsn4        information about a pod
        kubectl scale --replicas=0 deployment customer      this will make 0 replicas for customer pods..meaning you will not have any customer running after
        kubectl get svc                                     get the existing running services from kuberneted...the fraud and
                                                            notification having NodePort as TYPE means that they are only accesible
                                                            from another service inside kubernetes network...while customer is accesible from outside,
                                                            because its type is LoadBalancer

        minikube tunnel                                     so that you can access the accessible services from kubernetes;
                                                            you need to provide admin password for this

34. File from spring-microservices/kubernetes/minikube/bootstrap/rabbitmq
    were taken as they were...no explanation on that
    same thing for zipkin files

35. !!!! We no longer need Eureka Server for Service discovery, because kubernetes offers us that for free.
    So we disabled eureka server from the application.yml files, by mentioning the property
        enabled: false
    So obviously you can also delete the module, and the eureka related annotations, but we should leave those
    cause we might still want to run the app the old way.
TODO    !!!! kEEP in mind to remove the "enabled: false" mentioned above in that case !!!

36. Related to point 35, 2 new config files were added, clients-default.properties and
    clients-kube.properties, which depending on the spring profile set, enable those services
    to be accesed at various URLS. Check FraudClient and NotificationClient..you will see the
    configurable url there.
        Needed to add "SPRING_PROFILES_ACTIVE=default" to environment variables of the intellij
        application runners..
    Next if we comment out the services from docker-compose, and start just the 3 infrastructure
    containers, zipkin, postgres, rabbit mq, and run the apps from run configuration, they should
    still work.
TODO    Keep in mind that if you want to start the app, having each microservice in a docker container,
        you need to uncomment the services from docker-compose.yml

37. Kafka - distributed streaming platform
          - real time event driven application
          - fault tolerant, scalable,

          - difference between kafka and Rabbit MQ ? in kafka messages can stay "alive" for a minute,
          hour, day or forever, while in Rabbit MQ they dissapear after being consumed.

    (check kafka.png diagram)
38. Kafka connect source - pulls data from data sources into kafka
    Kafka connect sinks - takes data from kafka topics and puts it in your data source
    Kafka streams - used for data transformation, enriching, filtering, grouping, aggregating etc
                  - data goes both ways

39. Kafka can be obtaine from https://kafka.apache.org/quickstart
    I have it here : /Users/danteshte/Applications

    1.To start it, go inside the folder and run
        bin/zookeeper-server-start.sh config/zookeeper.properties
    2.From another command prompt, run
        bin/kafka-server-start.sh config/server.properties
    3.If you want to run in command prompt a kafka consumer, you can run :
        bin/kafka-console-consumer.sh --topic dante --from-beginning --bootstrap-server localhost:9092

    4. Run "spring-kafka" project ...and send request from postman to http://localhost:8080/api/v1/messages
    In my "spring-kafka" project, there is an exmple of a simple REST endpoint, that produces a message,
    publishes it on a kafka topic, and consumes it...this is done by running kafka locally like
    mentioned above
    In production one would use a "managed kafka"...cloud hosted one like AWS offers this..










































